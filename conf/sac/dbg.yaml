# @package _group_
# specific confics for the sac algorithm
reward_function: sub_graph_dice # leptin_data+sub_graph_dice #leptin_data #Reward function: sub_graph_dice, fully_supervised, defining_rules, defining_rules_lg,defining_rules_edge_based
s_subgraph: [2, 4, 8] # [8, 16, 32, 64] # subgraph sizes
entropy_range: [0.05, 2.0]
discount: 0.99  # discount factor (commonly referred to as \gamma in RL)
init_temperature: 0.1  # initial temperature in Gibbs distribution
temperature_regulation: optimized #constant/follow_quality/optimized
# Adam optim conf
alpha_lr: 0.0001
actor_lr: 0.0001
actor_update_frequency: 1  # optim step every n-th step
actor_update_after: 1  # update actor after critic has been warmed up
n_actions: 1
critic_lr: 0.0001
critic_tau: 0.005
critic_target_update_frequency: 5  # optim step every n-th step

use_closed_form_entropy: true  # uses closed form entropy in critic update (currently for normal distributions only)
sl_beta: 10  # weight for side loss
diag_gaussian_actor:  # specification of multinomial gaussian, acting as policy
  std_bounds: [0.01, 3.0]
  mu_bounds: [-5, 5]

#policy_warmup:  # the policy might need pretraining to make training faster
#  # Adam optim conf
#  lr: 1e-4
#  betas: [0.9, 0.999]
#
#  n_iterations: 10 # number of iterations of feature extrqactor warmup
#  batch_size: 2  # batch size for feature extractor warmup
#
#  patch_manager:
#    name: no_cross # rotated, no_cross, none
#    patch_shape: [256, 256]
#    patch_stride: [16, 16]
#    reorder_sp: true