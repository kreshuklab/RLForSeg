# @package _group_
# specific confics for the sac algorithm
reward_function: artificial_cells #Reward function: sub_graph_dice, artificial_cells
s_subgraph: [2, 4, 8] # [8, 16, 32, 64] # subgraph sizes
entropy_range: [0.05, 1.8]
discount: 0.99  # discount factor (commonly referred to as \gamma in RL)
init_temperature: 0.1  # initial temperature in Gibbs distribution
temperature_regulation: optimized #constant/follow_quality/optimized
# Adam optim conf
alpha_lr: 1e-4
actor_lr: 1e-4
actor_update_frequency: 1  # optim step every n-th step
actor_update_after: 150  # update actor after critic has been warmed up
n_actions: 1
critic_lr: 1e-4
critic_tau: 0.005
critic_target_update_frequency: 1  # optim step every n-th step

use_closed_form_entropy: true  # uses closed form entropy in critic update (currently for normal distributions only)
sl_beta: 10  # weight for side loss
diag_gaussian_actor:  # specification of multinomial gaussian, acting as policy
  std_bounds: [0.01, 5.0]
  mu_bounds: [-5, 5]
  sample_factor: 1.0  # samples before scaling and offset are in [0, 1]
  sample_offset: 0.0

#policy_warmup:  # the policy might need pretraining to make training faster
#  # Adam optim conf
#  lr: 1e-3
#
#  n_iterations: 500 # number of iterations of feature extrqactor warmup
#  batch_size: 1  # batch size for feature extractor warmup
#
#  patch_manager:
#    name: no_cross # rotated, no_cross, none
#    patch_shape: [512, 512]
#    patch_stride: [128, 128]
#    reorder_sp: true

